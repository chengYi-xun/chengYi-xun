<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>笔记｜生成模型（三）：生成对抗理论 | AAA高老庄旺铺招租的个人博客</title><meta name="author" content="AAA高老庄旺铺招租"><meta name="copyright" content="AAA高老庄旺铺招租"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="生成对抗网络（Generative Adversarial Nets，GAN） 核心思想 生成对抗网络是一种基于对抗学习的深度生成模型，最早由Ian Goodfellow于2014年在《Generative Adversarial Nets》中提出，一经提出便成为了学术界研究的热点，也将生成模型的热度推向了另一个新的高峰。上节有讨论到，直接用图片做监督存带来均值灾难，我们又无法得到真实分布从而监督">
<meta property="og:type" content="article">
<meta property="og:title" content="笔记｜生成模型（三）：生成对抗理论">
<meta property="og:url" content="https://chengyi-xun.github.io/chengYi-xun/posts/3-GAN-theory/index.html">
<meta property="og:site_name" content="AAA高老庄旺铺招租的个人博客">
<meta property="og:description" content="生成对抗网络（Generative Adversarial Nets，GAN） 核心思想 生成对抗网络是一种基于对抗学习的深度生成模型，最早由Ian Goodfellow于2014年在《Generative Adversarial Nets》中提出，一经提出便成为了学术界研究的热点，也将生成模型的热度推向了另一个新的高峰。上节有讨论到，直接用图片做监督存带来均值灾难，我们又无法得到真实分布从而监督">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chengyi-xun.github.io/chengYi-xun/img/avatar.png">
<meta property="article:published_time" content="2025-08-06T17:37:31.000Z">
<meta property="article:modified_time" content="2025-09-04T15:05:45.977Z">
<meta property="article:author" content="AAA高老庄旺铺招租">
<meta property="article:tag" content="Deep learning">
<meta property="article:tag" content="Generative models theory">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chengyi-xun.github.io/chengYi-xun/img/avatar.png"><link rel="shortcut icon" href="/chengYi-xun/img/favicon64x64.png"><link rel="canonical" href="https://chengyi-xun.github.io/chengYi-xun/posts/3-GAN-theory/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/chengYi-xun/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/chengYi-xun/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '笔记｜生成模型（三）：生成对抗理论',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-09-04 23:05:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/chengYi-xun/atom.xml" title="AAA高老庄旺铺招租的个人博客" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/chengYi-xun/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/chengYi-xun/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/chengYi-xun/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/chengYi-xun/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/chengYi-xun/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/chengYi-xun/img/top_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/chengYi-xun/" title="AAA高老庄旺铺招租的个人博客"><span class="site-name">AAA高老庄旺铺招租的个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/chengYi-xun/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/chengYi-xun/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">笔记｜生成模型（三）：生成对抗理论</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-06T17:37:31.000Z" title="发表于 2025-08-07 01:37:31">2025-08-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-04T15:05:45.977Z" title="更新于 2025-09-04 23:05:45">2025-09-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/chengYi-xun/categories/Notes/">Notes</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="笔记｜生成模型（三）：生成对抗理论"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2
id="生成对抗网络generative-adversarial-netsgan">生成对抗网络（Generative
Adversarial Nets，GAN）</h2>
<p><strong>核心思想</strong></p>
<p>生成对抗网络是一种基于对抗学习的深度生成模型，最早由Ian
Goodfellow于2014年在《Generative Adversarial
Nets》中提出，一经提出便成为了学术界研究的热点，也将生成模型的热度推向了另一个新的高峰。上节有讨论到，直接用图片做监督存带来均值灾难，我们又无法得到真实分布从而监督训练。因此，借助变分推断的思想做一个概率分布近似。从一个简单的已知分布（如标准高斯分布）出发，通过某种方式或手段，将其近似为真实数据的概率分布。GAN正是遵循这一理论，但实现过程中直接对齐分布是很难的，因为我们并不知道概率分布的函数形式，所以也无法得知它到底有几个参数。</p>
<p>所以可以换一个思想，既然无法得到概率分布函数的具体形式，没有参数，不好近似，那我就不去近似他了。对于两个分布而言，如果它们的大多数随机采样的<strong>样本概率</strong>都是对齐的，那不就说明这两个概率分布函数已经接近了吗。很好，你已经掌握了生成对抗网络的要领，试着自己实现一下吧。（-_-||）</p>
<p><strong>网络架构</strong></p>
<p>生成对抗网络采用双网络架构设计，由生成器（Generator,
G）和判别器（Discriminator,
D）两个神经网络组成，它们在训练过程中相互对抗、共同进化。</p>
<p><strong>生成器网络</strong>：作为整个系统的"创造者"，生成器的任务是学习从简单的噪声分布（通常是标准高斯分布）到复杂数据分布的映射关系。具体而言，它接收一个低维的随机噪声向量
<span class="math inline">\(z \sim p_z(z)\)</span>
作为输入，通过多层神经网络的非线性变换，输出与真实数据维度相同的生成样本
<span
class="math inline">\(G(z)\)</span>。生成器的目标是使生成的样本在分布上尽可能接近真实数据分布
<span class="math inline">\(p_{data}(x)\)</span>。</p>
<p><strong>判别器网络</strong>：作为系统的"鉴别专家"，判别器本质上是一个二分类器。它接收来自两个不同源的样本——真实数据集中的样本
<span class="math inline">\(x \sim p_{data}(x)\)</span>
和生成器产生的样本 <span
class="math inline">\(G(z)\)</span>，输出一个概率值 <span
class="math inline">\(D(x) \in
[0,1]\)</span>，表示输入样本来自真实数据分布的概率。判别器的目标是最大化正确分类的概率：对真实样本输出接近1，对生成样本输出接近0。</p>
<p><img src="/chengYi-xun/img/gan_net.png" alt="GAN Net" /></p>
<p>这种架构设计的巧妙之处在于，通过样本对齐来实现分布对齐。如果生成器产生的样本能够骗过判别器，说明生成分布已经在某种程度上接近了真实分布，而无需显式地建模分布函数。随着训练的进行，两个网络在对抗中相互提升，最终达到一个动态平衡状态。<strong>实现了样本概率之间的对齐，而不是样本对齐</strong>。（上一节说了直接图像对齐带来均值灾难，其本质原因是训练过程对齐的是样本而不是样本的概率）</p>
<h3 id="对抗训练机制">对抗训练机制</h3>
<p>GAN 的损失函数核心思想在于 <strong>对抗训练</strong>，生成器 G
和判别器 D 进行一场零和博弈：</p>
<ul>
<li><strong>生成器（Generator）</strong>：试图生成足以 "以假乱真"
的样本，欺骗判别器<br />
</li>
<li><strong>判别器（Discriminator）</strong>：试图准确区分真实样本和生成样本</li>
</ul>
<p>这种对抗关系可以用一个形象的比喻来理解：生成器就像一个造假币的人，而判别器就像一个验钞机。造假者不断提高造假技术，而验钞机也在不断提升鉴别能力。在这个博弈过程中，两者的能力都在不断提升。论文所述训练步骤如图所示。</p>
<p><img src="/chengYi-xun/img/gan_train.png" alt="GAN Train" /></p>
<p><strong>训练步骤</strong></p>
<ul>
<li><p>首先，使用交叉熵损失函数来训练判别器参数 <span
class="math inline">\(\theta_d\)</span>。从高斯分布 <span
class="math inline">\(p_g(z)\)</span> 中采样 <span
class="math inline">\(m\)</span> 个噪声向量：<span
class="math inline">\(\{ z^{(1)}, \ldots, z^{(m)}
\}\)</span>。从数据分布 <span
class="math inline">\(p_{\text{data}}(x)\)</span> 中采样 <span
class="math inline">\(m\)</span> 个真实样本：<span
class="math inline">\(\{ x^{(1)}, \ldots, x^{(m)}
\}\)</span>。使用交叉熵损失训练判别器网络 <span
class="math inline">\(\theta_d\)</span>：</p>
<p><span class="math display">\[
\mathop{\arg\min}_{\theta_d} \left( \frac{1}{m} \sum_{i=1}^{m} \left[
\log D(x^{(i)}) + \log \left( 1 - D(G(z^{(i)})) \right) \right] \right)
\]</span></p>
<p>从损失函数可以看出，目标是使 <span class="math inline">\(\log
D(x^{(i)})\)</span> 趋于 1，<span
class="math inline">\(D(G(z^{(i)}))\)</span> 趋于
0，以达到判别器识别生成数据的目的。</p></li>
<li><p>判别器更新后，固定判别器参数。重新再次采样 <span
class="math inline">\(m\)</span> 个噪声向量：<span
class="math inline">\(\{ z^{(1)}, \ldots, z^{(m)}
\}\)</span>。最小化判别损失以使生成器能够骗过判别器（即让生成的样本更像真实样本）：</p>
<p><span class="math display">\[
\mathop{\arg\min}_{\theta_g} \left( \frac{1}{m} \sum_{i=1}^{m} \left[
\log \left( 1 - D(G(z^{(i)})) \right) \right] \right)
\]</span></p>
<blockquote>
<p>💡 在实际应用中，为了提升生成器的梯度，也常使用替代形式：<span
class="math inline">\(-\log(D(G(z)))\)</span>，其目标是让判别器认为生成样本为真。</p>
</blockquote></li>
<li><p>重复以上步骤，直到判别器和生成器都收敛。</p></li>
</ul>
<p>最后，整个训练的损失函数可以整合为对抗损失函数（Adversarial Loss
Function / Minimax Objective）</p>
<p><span class="math display">\[
\min_G \max_D \; V(D, G) = E_{x \sim p_{\text{data}}(x)} \left[ \log
D(x) \right] + E_{z \sim p_g(z)} \left[ \log \left( 1 - D(G(z)) \right)
\right]
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(D(x)\)</span> 表示判别器对真实样本 <span
class="math inline">\(x\)</span> 判定为真的概率；</li>
<li><span class="math inline">\(D(G(z))\)</span>
表示判别器对生成样本为真的概率；</li>
<li><span class="math inline">\(G(z)\)</span> 是生成器对输入噪声 <span
class="math inline">\(z\)</span> 的映射输出；</li>
<li><span class="math inline">\(E_{x \sim p_{\text{data}}(x)}\)</span>
是对真实数据分布的期望；</li>
<li><span class="math inline">\(E_{z \sim p_g(z)}\)</span>
是对生成器输入噪声分布的期望。</li>
</ul>
<h2
id="gan能达到平衡的原因以及背后的数学证明">GAN能达到平衡的原因以及背后的数学证明</h2>
<p><strong>平衡原因的直观解释</strong></p>
<p>这里首先给出作者画的图来解释为什么对抗训练能够收敛。图中绿色线代表生成器的概率分布，蓝色虚线代表判别器的分布，黑色虚线代表真实数据分布。字母
<span class="math inline">\(z\)</span> 代表采样的高斯噪声，<span
class="math inline">\(x\)</span> 代表真实数据，我们从 <span
class="math inline">\(z\)</span> 中采样，通过生成器将 <span
class="math inline">\(z\)</span> 的样本映射成符合 <span
class="math inline">\(x\)</span> 分布的样本。在图<span
class="math inline">\((a)\)</span>
中，判别器和生成器的概率分布都是随机设定，大部分样本的映射都不正确。首先训练判别器，既让判别器能够识别出哪些是映射过来的数据，哪些是真实样本数据。训练完成之后就是图<span
class="math inline">\((b)\)</span>
所展示的分布。真实数据判别接近1，生成数据判别接近0。然后再训练生成器，让生成器朝着让判别器无法判别的方向移动，即为图<span
class="math inline">\((c)\)</span> ，最后经过循环拉扯，达到了图<span
class="math inline">\((d)\)</span>
的平衡。即生成器的分布和真实数据的分布相同。</p>
<p><img src="/chengYi-xun/img/gan_theroy.png" alt="GAN theroy" /></p>
<p><strong>背后的数学原理</strong></p>
<p>在深入理解GAN的数学基础之前，我们需要先回顾KL散度（Kullback-Leibler
Divergence）的两个关键局限性：</p>
<p><strong>KL散度的局限性：</strong></p>
<p>之前我们有讨论到KL散度的非负性，其实它还具有其他两个特性，即：</p>
<ol type="1">
<li><p><strong>非对称性</strong>：KL散度不满足对称性 <span
class="math display">\[KL(P\|Q) \neq KL(Q\|P)\]</span></p></li>
<li><p><strong>数值不稳定性</strong>：当两个分布的支撑集不重叠时，KL散度趋向于无穷大</p></li>
</ol>
<p><span class="math display">\[\text{当 } P(x) &gt; 0 \text{ 且 } Q(x)
= 0 \text{ 时，} KL(P\|Q) \to +\infty\]</span> <span
class="math display">\[\text{当 } Q(x) &gt; 0 \text{ 且 } P(x) = 0
\text{ 时，} KL(P\|Q) \to -\infty\]</span></p>
<p>这两个问题在GAN训练中会导致严重的梯度消失和数值不稳定问题。因此，引入Jensen-Shannon散度（JS散度）作为更适合的距离度量。</p>
<p><strong>JS 散度</strong></p>
<p><span class="math display">\[
JS(P \| Q) = \frac{1}{2} KL(P \| M) + \frac{1}{2} KL(Q \| M)
\]</span> 其中 <span class="math inline">\(M = \frac{1}{2}(P +
Q)\)</span> 。</p>
<p><strong>积分形式</strong> <span class="math display">\[
JS(P\|Q) = \frac{1}{2}\int p(x)\log\frac{2p(x)}{p(x)+q(x)}dx +
\frac{1}{2}\int q(x)\log\frac{2q(x)}{p(x)+q(x)}dx
\]</span></p>
<p><strong>JS散度的重要特性：</strong></p>
<ol type="1">
<li><p><strong>对称性</strong>：<span class="math inline">\(JS(P\|Q) =
JS(Q\|P)\)</span></p></li>
<li><p><strong>非负性</strong>：<span class="math inline">\(JS(P\|Q)
\geq 0\)</span>，当且仅当 <span class="math inline">\(P = Q\)</span>
时等号成立</p></li>
<li><p><strong>有界性</strong>：<span class="math inline">\(0 \leq
JS(P\|Q) \leq \log 2\)</span></p></li>
<li><p><strong>平滑性</strong>：相比KL散度，JS散度在处理不重叠分布时更加稳定</p></li>
</ol>
<p>但其实以上两种概率分布度量方式都有局限性，在 <span
class="math inline">\(PQ\)</span>
不重叠的情况下，KL散度趋于无穷，这会导致梯度爆炸，而JS散度趋于常值，导致梯度消失。更多感兴趣的可以看：<a
target="_blank" rel="noopener" href="https://blog.csdn.net/Invokar/article/details/88917214">两者分布不重合JS散度为log2的数学证明</a></p>
<p>另外，我们再介绍一下狄拉克函数：</p>
<p><strong>狄拉克函数 <span class="math inline">\(\delta(x)\)</span>
与生成器的概率分布 <span class="math inline">\(p_g(x)\)</span>
表示</strong></p>
<p>狄拉克函数 <span class="math inline">\(\delta(x)\)</span>
并不是数学中一个严格意义上的函数，而是在泛函分析中被称为广义函数（generalized
function）或分布（distribution），它在除零以外的点上都等于零，且其在整个定义域上的积分等于1。用数学定义的方式可以写为：
<span class="math display">\[
\delta(x) =
\begin{cases}
0, &amp; x \neq 0, \\
\infty, &amp; x = 0,
\end{cases}
\quad\text{且}\quad
\int_{-\infty}^{\infty} \delta(x) \, dx = 1.
\]</span></p>
<p>以上定义方式并不严谨，感兴趣的朋友请移步泛函分析，（再展开就没完没了了）。它具有抽样性质（Sifting
Property）：</p>
<p><span class="math display">\[
\int_{-\infty}^{\infty} f(x) \, \delta(x-a) \, dx = f(a).
\]</span></p>
<p>没错，学过信号系统的同学立马反应过来了，<strong>这他么不就是脉冲函数吗</strong>！是的，只是在不同领域叫法不同。</p>
<p>生成器 <span class="math inline">\(G\)</span> 将噪声 <span
class="math inline">\(z \sim p_z(z)\)</span> 映射为样本 <span
class="math inline">\(x\)</span>，那么生成的样本 <span
class="math inline">\(x\)</span> 的概率密度可以通过以下方式计算：</p>
<p><span class="math display">\[p_g(x) = \int_z
p_z(z)\delta(x-G(z))dz\]</span> 对于确定性映射 <span
class="math inline">\(G\)</span>，只有在 <span class="math inline">\(x =
G(z)\)</span> 的某个 <span class="math inline">\(z\)</span>
值处才有非零概率密度。狄拉克δ函数 <span
class="math inline">\(\delta(x-G(z))\)</span> 正好捕获了这一点：</p>
<ul>
<li>当 <span class="math inline">\(x = G(z)\)</span> 时，<span
class="math inline">\(\delta(x-G(z)) = \infty\)</span></li>
<li>当 <span class="math inline">\(x \neq G(z)\)</span> 时，<span
class="math inline">\(\delta(x-G(z)) = 0\)</span></li>
</ul>
<p>意思是我们先按 <span class="math inline">\(p_z(z)\)</span>
采样一个潜变量 <span class="math inline">\(z\)</span>，生成器 <span
class="math inline">\(G(z)\)</span> 会把这个 <span
class="math inline">\(z\)</span> 映射成一个数据样本 <span
class="math inline">\(x&#39; = G(z)\)</span>，δ 函数 <span
class="math inline">\(\delta(x - G(z))\)</span> 表示：只有当 <span
class="math inline">\(x\)</span> 恰好等于 <span
class="math inline">\(G(z)\)</span>
时（生成的样本符合真实图像概率分布），这个积分才有贡献，所以积分结果就是——<strong>所有能生成
<span class="math inline">\(x\)</span> 的 <span
class="math inline">\(z\)</span> 的概率质量总和</strong>。</p>
<p>也就是说，<span class="math inline">\(p_g(x)\)</span> 是
<strong>通过生成器将潜空间的分布 <span
class="math inline">\(p_z(z)\)</span>
推送（pushforward）到数据空间后得到的分布</strong>。</p>
<p><strong>更数学化的理解（推送分布）</strong></p>
<p>这个公式本质上是
<strong>概率分布的变换公式</strong>，在测度论的语言里就是： <span
class="math display">\[
p_g = G_\# p_z
\]</span> 其中 <span class="math inline">\(G_\#\)</span> 表示
<strong>推送测度</strong>（pushforward measure）——用生成器 <span
class="math inline">\(G\)</span> 把 <span
class="math inline">\(z\)</span> 空间的概率分布搬到 <span
class="math inline">\(x\)</span>
空间。如果生成器是确定性的（没有噪声），那么：</p>
<ul>
<li>每个 <span class="math inline">\(z\)</span> 只会映射到一个 <span
class="math inline">\(x\)</span></li>
<li>δ 函数就表示了这种确定映射关系</li>
</ul>
<p>举个简单例子</p>
<p>假设：</p>
<ul>
<li><span class="math inline">\(p_z(z)\)</span> 是均匀分布在区间 <span
class="math inline">\([0,1]\)</span></li>
<li><span class="math inline">\(G(z) = 2z\)</span></li>
</ul>
<p>那么：</p>
<p><span class="math display">\[
p_g(x) = \int_{z=0}^1 1 \cdot \delta(x - 2z) \, dz
\]</span></p>
<p>用 δ 函数的性质，解得：</p>
<p><span class="math display">\[
p_g(x) = \frac{1}{2} \quad \text{当 } x \in [0, 2] \quad \text{否则 } 0
\]</span></p>
<p>这正是把 <span class="math inline">\([0,1]\)</span>
的均匀分布线性拉伸到 <span class="math inline">\([0,2]\)</span>
后的概率密度。</p>
<p><strong>回归GAN的正题，在理想情况下，当GAN训练达到纳什均衡时</strong>：</p>
<ul>
<li>判别器无法区分真假样本，即 <span class="math inline">\(D(x) =
0.5\)</span></li>
<li>生成器的生成分布与真实数据分布一致，即 <span
class="math inline">\(p_g = p_{data}\)</span></li>
</ul>
<p>用数学表达有：</p>
<ul>
<li>对于固定生成器 <span
class="math inline">\(G\)</span>，判别器的最优解为 <span
class="math display">\[
D^*(x)=\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}.
\]</span></li>
</ul>
<p>把该最优判别器代回目标函数后，生成器的目标等价于最小化真实分布与生成分布之间的JS散度：
<span class="math display">\[
  C(G)\equiv V\big(G,D^*\big) = -\log 4 +
2\,\mathrm{JS}\big(p_{data}\parallel p_g\big).
\]</span></p>
<p>因为 <span class="math inline">\(JS \ge 0\)</span> 且当且仅当 <span
class="math inline">\(p_g=p_{data}\)</span> 时为 0，所以在纳什均衡处
<span class="math inline">\(p_g=p_{data}\)</span>，此时 <span
class="math inline">\(D^*(x)=\tfrac{1}{2}\)</span>。</p>
<h4 id="详细证明">详细证明</h4>
<p><strong>步骤1：求解最优判别器</strong></p>
<p>对于固定的生成器G，判别器D的目标是最大化：</p>
<p><span class="math display">\[V(G,D) = E_{x \sim p_{data}}[\log D(x)]
+ E_{z \sim p_z}[\log(1-D(G(z)))]\]</span></p>
<p>其中 <span class="math inline">\(E\)</span>
表示数学期望，展开得：</p>
<p><span class="math display">\[V(G,D) = \int_x p_{data}(x)\log D(x)dx +
\int_z p_z(z)\log(1-D(G(z)))dz\]</span></p>
<p>因此把第二项用狄拉克函数展开有：<br />
<span class="math display">\[
  \log(1 - D(G(z))) = \int_x \log(1 - D(x)) \, \delta\big(x - G(z)\big)
\, dx.
\]</span> 把上面的表达式代回原积分：</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\int_z p_z(z) \log(1 - D(G(z))) \, dz \\
&amp;= \int_z p_z(z) \left[ \int_x \log(1 - D(x)) \, \delta(x - G(z)) \,
dx \right] dz.
\end{aligned}
\]</span></p>
<p>交换积分顺序（Fubini 定理，在广义函数意义下成立）：</p>
<p><span class="math display">\[
= \int_x \log(1 - D(x)) \left[ \int_z p_z(z) \, \delta(x - G(z)) \, dz
\right] dx.
\]</span></p>
<p>又因为 <span class="math inline">\(p_g(x) = \int_z
p_z(z)\delta(x-G(z))dz\)</span>，代入有：</p>
<p><span class="math display">\[V(G,D) = \int_x p_{data}(x)\log D(x) +
p_g(x)\log(1-D(x))dx\]</span></p>
<p>写成期望的形式： <span class="math display">\[V(G,D) = E_{x \sim
p_{data}}[\log D(x)] + E_{x \sim p_g}[\log(1-D(x))]\]</span></p>
<p>注意：<span class="math inline">\(D(x)\)</span> 在每个 <span
class="math inline">\(x\)</span> 处独立出现，因此对于固定的 <span
class="math inline">\(G\)</span>，求使 <span
class="math inline">\(V(G,D)\)</span> 最大化的 <span
class="math inline">\(D\)</span> 可以逐点（对每个 <span
class="math inline">\(x\)</span>）独立求解，因此可以通过求被积函数的极值点来最大化概率。</p>
<p><span class="math display">\[\text{令} f(D) = p_{data}(x)\log D(x) +
p_g(x)\log(1-D(x))\]</span></p>
<p>对D求导并令其为0： <span class="math display">\[\frac{\partial
f}{\partial D} = \frac{p_{data}(x)}{D(x)} - \frac{p_g(x)}{1-D(x)} =
0\]</span></p>
<p>二阶导数为</p>
<p><span class="math display">\[\frac{\partial^2 f}{\partial D^2} =
-\frac{p_{data}(x)}{D(x)^2} - \frac{p_g(x)}{(1-D(x))^2} &lt;
0\]</span></p>
<p>因此该临界点是全局最大值（对每个 <span
class="math inline">\(x\)</span>）。若 <span
class="math inline">\(p_{data}(x)=p_g(x)=0\)</span>，则该点在积分意义上不影响结果（可任意定义）。</p>
<p>解得最优判别器： <span class="math display">\[D^*_G(x) =
\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\]</span></p>
<p><strong>步骤2：将最优判别器代回目标函数</strong></p>
<p>将 <span class="math inline">\(D^*_G(x)\)</span> 代入原始目标函数：
<span class="math display">\[
\begin{aligned}
C(G) \equiv V\big(G,D^*\big)
&amp;= \int \Big[ p_{data}(x)\log D^*(x) + p_g(x)\log\big(1-D^*(x)\big)
\Big] dx \\
&amp;= \int \Big[ p_{data}\log\frac{p_{data}}{p_{data}+p_g}
            + p_g\log\frac{p_g}{p_{data}+p_g} \Big] dx \\
&amp;= \int \Big[ p_{data}\log p_{data} + p_g\log p_g
            - (p_{data}+p_g)\log(p_{data}+p_g) \Big] dx \\
&amp;= E_{x\sim p_{data}}\left[\log\frac{p_{data}(x)}{p_{data}(x) +
p_g(x)}\right] + E_{x\sim p_g}\left[\log\frac{p_g(x)}{p_{data}(x) +
p_g(x)}\right]
\end{aligned}
\]</span></p>
<p><strong>步骤3：变换为JS散度</strong></p>
<p>注意到： <span
class="math display">\[\log\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} =
\log\frac{p_{data}(x)}{2 \cdot \frac{p_{data}(x) + p_g(x)}{2}} =
\log\frac{p_{data}(x)}{2M(x)} = \log 2 +
\log\frac{p_{data}(x)}{M(x)}\]</span></p>
<p>其中 <span class="math inline">\(M(x) = \frac{p_{data}(x) +
p_g(x)}{2}\)</span> 是两个分布的平均。</p>
<p>因此： <span class="math display">\[C(G) = \log 4 + KL(p_{data} \| M)
+ KL(p_g \| M)\]</span></p>
<p>根据JS散度的定义：</p>
<p><span class="math display">\[JS(p_{data} \| p_g) =
\frac{1}{2}KL(p_{data} \| M) + \frac{1}{2}KL(p_g \| M)\]</span></p>
<p>我们得到</p>
<p><span class="math display">\[C(G) = \log 4 + 2 \cdot JS(p_{data} \|
p_g)\]</span></p>
<p>由于生成器G要最小化 <span class="math inline">\(C(G)\)</span>，而
<span class="math inline">\(\log 4\)</span> 是常数，因此： <span
class="math display">\[\min_G C(G) \Leftrightarrow \min_G JS(p_{data} \|
p_g)\]</span></p>
<p>根据JS散度的性质和公式，有： 1. 当且仅当 <span
class="math inline">\(p_g=p_{data}\)</span>
时，JS散度为0，达到全局最小值 2. 此时最优判别器 <span
class="math inline">\(D^*_G(x) = \frac{1}{2}\)</span>，无法区分真假样本
3. 这证明了GAN在理论上能够学习到真实数据分布</p>
<p>这个证明揭示了几个重要事实：</p>
<ol type="1">
<li><p><strong>GAN隐式地最小化JS散度</strong>：虽然我们没有显式计算两个分布之间的距离，但通过对抗训练，实际上在最小化JS散度</p></li>
<li><p><strong>判别器的作用</strong>：判别器不仅仅是一个分类器，它实际上在估计两个分布的密度比</p></li>
<li><p><strong>收敛性保证</strong>：在理想条件下（无限容量、充分训练），GAN能够收敛到真实数据分布</p></li>
</ol>
<p>然而，这个理论分析基于几个理想假设（如无限容量的模型、全局最优等），在实践中难以满足，这也是GAN训练不稳定的原因之一。</p>
<h3 id="gan的优势与挑战">GAN的优势与挑战</h3>
<p><strong>优势：</strong></p>
<ol type="1">
<li><strong>无需显式建模概率密度</strong>：不需要像VAE那样引入变分下界</li>
<li><strong>生成质量高</strong>：能够生成非常逼真的样本</li>
<li><strong>理论优雅</strong>：基于博弈论的框架简洁而有力</li>
</ol>
<p><strong>挑战：</strong></p>
<ol type="1">
<li><strong>训练不稳定</strong>：生成器和判别器的平衡很难把握</li>
<li><strong>模式坍塌（Mode
Collapse）</strong>：生成器可能只学会生成某几种模式的样本</li>
<li><strong>梯度消失</strong>：当判别器过于强大时，生成器的梯度会消失</li>
<li><strong>评估困难</strong>：难以定量评估生成模型的质量</li>
</ol>
<h3 id="gan的变体与改进">GAN的变体与改进</h3>
<p>为了解决原始GAN的问题，研究者们提出了许多改进版本：</p>
<ol type="1">
<li><strong>DCGAN（Deep Convolutional
GAN）</strong>：使用卷积神经网络架构，提高了训练稳定性</li>
<li><strong>WGAN（Wasserstein
GAN）</strong>：使用Wasserstein距离替代JS散度，缓解了梯度消失问题</li>
<li><strong>LSGAN（Least Squares
GAN）</strong>：使用最小二乘损失，使生成样本更接近决策边界</li>
<li><strong>StyleGAN</strong>：引入风格控制，实现了高质量、可控的图像生成</li>
<li><strong>CycleGAN</strong>：实现了无配对数据的图像到图像转换</li>
</ol>
<h3 id="总结">总结</h3>
<p>GAN开创了生成模型的新时代，通过对抗学习的思想，巧妙地避开了直接建模概率分布的困难。虽然训练过程存在诸多挑战，但其强大的生成能力使其在图像生成、风格迁移、超分辨率等多个领域取得了巨大成功。随着各种改进技术的出现，GAN已经成为深度学习中最重要的生成模型之一。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/chengYi-xun">AAA高老庄旺铺招租</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://chengyi-xun.github.io/chengYi-xun/posts/3-GAN-theory/">https://chengyi-xun.github.io/chengYi-xun/posts/3-GAN-theory/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://chengyi-xun.github.io/chengYi-xun" target="_blank">AAA高老庄旺铺招租的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/chengYi-xun/tags/Deep-learning/">Deep learning</a><a class="post-meta__tags" href="/chengYi-xun/tags/Generative-models-theory/">Generative models theory</a></div><div class="post_share"><div class="social-share" data-image="/chengYi-xun/img/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/chengYi-xun/posts/4-VAE-theory/" title="笔记｜生成模型（四）：变分自编码器"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">笔记｜生成模型（四）：变分自编码器</div></div></a></div><div class="next-post pull-right"><a href="/chengYi-xun/posts/2-generation-model/" title="笔记｜生成模型（二）：生成模型的技术路线总览"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">笔记｜生成模型（二）：生成模型的技术路线总览</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/chengYi-xun/posts/9-SDE/" title="笔记｜生成模型（八）：SDE统一DDPM和SMLD"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-21</div><div class="title">笔记｜生成模型（八）：SDE统一DDPM和SMLD</div></div></a></div><div><a href="/chengYi-xun/posts/8-score-match/" title="笔记｜生成模型（七）：Score-Base理论"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-16</div><div class="title">笔记｜生成模型（七）：Score-Base理论</div></div></a></div><div><a href="/chengYi-xun/posts/7-ddim/" title="笔记｜生成模型（六）：DDIM理论"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-12</div><div class="title">笔记｜生成模型（六）：DDIM理论</div></div></a></div><div><a href="/chengYi-xun/posts/6-ddpm/" title="笔记｜生成模型（五）：DDPM理论"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-10</div><div class="title">笔记｜生成模型（五）：DDPM理论</div></div></a></div><div><a href="/chengYi-xun/posts/4-VAE-theory/" title="笔记｜生成模型（四）：变分自编码器"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-09</div><div class="title">笔记｜生成模型（四）：变分自编码器</div></div></a></div><div><a href="/chengYi-xun/posts/2-generation-model/" title="笔记｜生成模型（二）：生成模型的技术路线总览"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-04</div><div class="title">笔记｜生成模型（二）：生成模型的技术路线总览</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="disqusjs-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/chengYi-xun/img/avatar.png" onerror="this.onerror=null;this.src='/chengYi-xun/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">AAA高老庄旺铺招租</div><div class="author-info__description">欢迎来到我的博客</div></div><div class="card-info-data site-data is-center"><a href="/chengYi-xun/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/chengYi-xun/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/chengYi-xun/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chengYi-xun"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chengYi-xun" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ldq4399@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/chengYi-xun/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到AAA高老庄旺铺招租的猪舍~（昼伏夜出型）</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9Cgenerative-adversarial-netsgan"><span class="toc-number">1.</span> <span class="toc-text">生成对抗网络（Generative
Adversarial Nets，GAN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.</span> <span class="toc-text">对抗训练机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gan%E8%83%BD%E8%BE%BE%E5%88%B0%E5%B9%B3%E8%A1%A1%E7%9A%84%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AF%81%E6%98%8E"><span class="toc-number">2.</span> <span class="toc-text">GAN能达到平衡的原因以及背后的数学证明</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E8%AF%81%E6%98%8E"><span class="toc-number">2.0.1.</span> <span class="toc-text">详细证明</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gan%E7%9A%84%E4%BC%98%E5%8A%BF%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number">2.1.</span> <span class="toc-text">GAN的优势与挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gan%E7%9A%84%E5%8F%98%E4%BD%93%E4%B8%8E%E6%94%B9%E8%BF%9B"><span class="toc-number">2.2.</span> <span class="toc-text">GAN的变体与改进</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.3.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-post-series"><div class="item-headline"><i class="fa-solid fa-layer-group"></i><span>系列文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/7-ddim/" title="笔记｜生成模型（六）：DDIM理论">笔记｜生成模型（六）：DDIM理论</a><time datetime="2025-08-12T15:08:30.000Z" title="发表于 2025-08-12 23:08:30">2025-08-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/6-ddpm/" title="笔记｜生成模型（五）：DDPM理论">笔记｜生成模型（五）：DDPM理论</a><time datetime="2025-08-10T15:08:30.000Z" title="发表于 2025-08-10 23:08:30">2025-08-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/4-VAE-theory/" title="笔记｜生成模型（四）：变分自编码器">笔记｜生成模型（四）：变分自编码器</a><time datetime="2025-08-08T17:37:31.000Z" title="发表于 2025-08-09 01:37:31">2025-08-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/3-GAN-theory/" title="笔记｜生成模型（三）：生成对抗理论">笔记｜生成模型（三）：生成对抗理论</a><time datetime="2025-08-06T17:37:31.000Z" title="发表于 2025-08-07 01:37:31">2025-08-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/2-generation-model/" title="笔记｜生成模型（二）：生成模型的技术路线总览">笔记｜生成模型（二）：生成模型的技术路线总览</a><time datetime="2025-08-03T17:37:31.000Z" title="发表于 2025-08-04 01:37:31">2025-08-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/1-generation-basic-theory/" title="笔记｜生成模型（一）：一些概率论的基础概念和理论">笔记｜生成模型（一）：一些概率论的基础概念和理论</a><time datetime="2025-07-30T17:37:31.000Z" title="发表于 2025-07-31 01:37:31">2025-07-31</time></div></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/10-classifier-guidance-for-diffusion-models/" title="笔记｜生成模型（九）：Classifier Guidance 理论与实现">笔记｜生成模型（九）：Classifier Guidance 理论与实现</a><time datetime="2025-08-22T09:40:51.000Z" title="发表于 2025-08-22 17:40:51">2025-08-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/11-classifier-free-guidance-for-diffusion-models/" title="笔记｜生成模型（十）：Classifier-Free Guidance 理论与实现">笔记｜生成模型（十）：Classifier-Free Guidance 理论与实现</a><time datetime="2025-08-22T08:27:55.000Z" title="发表于 2025-08-22 16:27:55">2025-08-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/9-SDE/" title="笔记｜生成模型（八）：SDE统一DDPM和SMLD">笔记｜生成模型（八）：SDE统一DDPM和SMLD</a><time datetime="2025-08-21T15:08:30.000Z" title="发表于 2025-08-21 23:08:30">2025-08-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/8-score-match/" title="笔记｜生成模型（七）：Score-Base理论">笔记｜生成模型（七）：Score-Base理论</a><time datetime="2025-08-16T15:08:30.000Z" title="发表于 2025-08-16 23:08:30">2025-08-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/chengYi-xun/posts/7-ddim/" title="笔记｜生成模型（六）：DDIM理论">笔记｜生成模型（六）：DDIM理论</a><time datetime="2025-08-12T15:08:30.000Z" title="发表于 2025-08-12 23:08:30">2025-08-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 By AAA高老庄旺铺招租</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/chengYi-xun/js/utils.js"></script><script src="/chengYi-xun/js/main.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initDisqusjs = () => {
    window.disqusjs = null
    disqusjs = new DisqusJS(Object.assign({
      shortname: 'daniuniu',
      identifier: '/chengYi-xun/posts/3-GAN-theory/',
      url: 'https://chengyi-xun.github.io/chengYi-xun/posts/3-GAN-theory/',
      title: '笔记｜生成模型（三）：生成对抗理论',
      apikey: 'Ddx35LNYqc5BNc1ccxfuPQ51mkafhJ4OMIm3XPF8lQ6Jlx1s2Q7OgpMUENyJ2rRw',
    },null))

    disqusjs.render(document.getElementById('disqusjs-wrap'))
  }

  const themeChange = () => {
    const ele = document.getElementById('disqus_thread')
    if(!ele) return
    disqusjs.destroy()
    initDisqusjs()
  }

  btf.addGlobalFn('themeChange', themeChange, 'disqusjs')

  const loadDisqusjs = async() => {
    if (window.disqusJsLoad) initDisqusjs()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/disqusjs/dist/browser/styles/disqusjs.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/disqusjs/dist/browser/disqusjs.es2015.umd.min.js')
      initDisqusjs()
      window.disqusJsLoad = true
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqusjs-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=daniuniu&api_key=Ddx35LNYqc5BNc1ccxfuPQ51mkafhJ4OMIm3XPF8lQ6Jlx1s2Q7OgpMUENyJ2rRw&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()
      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Disqusjs' === 'Disqusjs' || !false) {
    if (false) btf.loadComment(document.getElementById('disqusjs-wrap'), loadDisqusjs)
    else {
      loadDisqusjs()
      
    }
  } else {
    window.loadOtherComment = loadDisqusjs
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
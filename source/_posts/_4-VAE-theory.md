---
title: 笔记｜生成模型（四）：变分自编码器
date: 2025-08-09 01:37:31
cover: false
mathjax: true
categories:
 - Notes
tags:
 - Deep learning
 - Generative models theory
series: Diffusion Models theory
---

## 变分自编码器（Variational Autoencoder，VAE）

**核心思想**

变分自编码器是一种基于概率生成模型的深度学习架构，由Kingma和Welling在2013年的论文《Auto-Encoding Variational Bayes》中提出。与GAN通过对抗训练来隐式学习数据分布不同，VAE采用了一种更加直接的方式：通过最大化证据下界（ELBO）来显式地学习数据的概率分布。

回顾我们在[第一篇文章](../1-generation-basic-theory)中介绍的变分推断和ELBO的概念，VAE正是这些理论的一个优雅应用。它巧妙地结合了深度学习的表示能力和贝叶斯推断的理论框架，实现了可控的、有理论保证的生成模型。

**问题的提出**

假设我们有一组观测数据 $x$（比如图像），我们想要学习一个生成模型 $p_\theta(x)$ 来近似真实的数据分布 $p(x)$。一个自然的想法是引入隐变量 $z$，它可以理解为数据的潜在表示或特征。那么，数据的边际似然可以写为：

$$p_\theta(x) = \int p_\theta(x|z)p(z)dz$$

这里 $p(z)$ 是隐变量的先验分布（通常选择标准高斯分布），$p_\theta(x|z)$ 是给定隐变量下数据的条件分布。

然而，这个积分通常是难以计算的（intractable），因为：
1. 积分需要对所有可能的 $z$ 进行，计算复杂度极高
2. 后验分布 $p_\theta(z|x)$ 通常没有解析形式

这就是VAE要解决的核心问题：如何在无法直接计算边际似然和后验分布的情况下，有效地训练一个生成模型？

## VAE的架构设计

VAE采用了一个编码器-解码器的架构，但与传统的自编码器不同，VAE的每个组件都有明确的概率解释：

**编码器（Encoder/Recognition Network）**：
- 输入：观测数据 $x$
- 输出：隐变量的后验分布参数
- 功能：学习一个近似后验分布 $q_\phi(z|x)$ 来逼近真实后验 $p_\theta(z|x)$
- 实现：通常输出高斯分布的均值 $\mu_\phi(x)$ 和标准差 $\sigma_\phi(x)$

**解码器（Decoder/Generative Network）**：
- 输入：隐变量 $z$
- 输出：重构数据 $\hat{x}$
- 功能：学习条件分布 $p_\theta(x|z)$
- 实现：根据任务不同，可以是伯努利分布（二值数据）或高斯分布（连续数据）

整个架构可以表示为：
$$x \xrightarrow{\text{Encoder}} q_\phi(z|x) \xrightarrow{\text{Sample}} z \xrightarrow{\text{Decoder}} p_\theta(x|z)$$

## 数学推导：从ELBO到VAE损失函数

让我们从最大化对数似然开始推导。对于观测数据 $x$，我们想要最大化：

$$\log p_\theta(x)$$

根据我们在第一篇中推导的ELBO公式：

$$\log p_\theta(x) = \text{ELBO} + \text{KL}(q_\phi(z|x) \| p_\theta(z|x))$$

由于KL散度非负，我们有：

$$\log p_\theta(x) \geq \text{ELBO}$$

因此，最大化ELBO等价于最大化对数似然的下界。让我们展开ELBO：

$$
\begin{aligned}
\text{ELBO} &= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x,z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] \\
&= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z) + \log p(z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] \\
&= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)}[\log p(z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] \\
&= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \text{KL}(q_\phi(z|x) \| p(z))
\end{aligned}
$$

因此，VAE的损失函数可以写为：

$$\mathcal{L}_{\text{VAE}}(\theta, \phi; x) = -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \text{KL}(q_\phi(z|x) \| p(z))$$

这个损失函数有两个非常直观的部分：

1. **重构损失（Reconstruction Loss）**：$-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$
   - 衡量解码器重构输入的能力
   - 对于伯努利分布，这等价于二元交叉熵
   - 对于高斯分布，这等价于均方误差

2. **正则化项（Regularization Term）**：$\text{KL}(q_\phi(z|x) \| p(z))$
   - 约束编码器输出的分布接近先验分布
   - 防止编码器简单地记忆训练数据
   - 确保隐空间的连续性和可插值性

## 重参数化技巧（Reparameterization Trick）

VAE训练中的一个关键挑战是：如何通过采样操作进行反向传播？因为采样是一个随机过程，梯度无法直接通过。

重参数化技巧巧妙地解决了这个问题。对于高斯分布 $q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \sigma^2_\phi(x))$，我们可以将采样过程重写为：

$$z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon$$

其中 $\epsilon \sim \mathcal{N}(0, I)$ 是从标准高斯分布采样的噪声，$\odot$ 表示逐元素乘积。

这样，随机性被转移到了与参数无关的 $\epsilon$ 上，而 $z$ 关于参数 $\phi$ 是可微的，从而可以使用标准的反向传播算法。

**为什么这个技巧如此重要？**

1. **可微性**：将不可微的采样操作转化为可微的确定性变换
2. **低方差**：相比于其他梯度估计方法（如REINFORCE），提供了低方差的梯度估计
3. **简单高效**：实现简单，计算效率高

## KL散度的解析形式

当先验分布 $p(z) = \mathcal{N}(0, I)$ 且近似后验 $q_\phi(z|x) = \mathcal{N}(\mu, \sigma^2)$ 时，KL散度有解析解：

$$\text{KL}(q_\phi(z|x) \| p(z)) = \frac{1}{2} \sum_{j=1}^{J} \left( 1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2 \right)$$

其中 $J$ 是隐变量的维度。这个解析形式使得VAE的训练非常高效。

## VAE的训练算法

综合上述推导，VAE的训练算法可以总结为：

1. **前向传播**：
   - 输入数据 $x$ 通过编码器得到 $\mu_\phi(x)$ 和 $\sigma_\phi(x)$
   - 使用重参数化技巧采样 $z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon$
   - $z$ 通过解码器得到重构 $\hat{x} = p_\theta(x|z)$

2. **计算损失**：
   - 重构损失：根据数据类型选择合适的损失函数
   - KL损失：使用解析公式计算
   - 总损失 = 重构损失 + $\beta$ × KL损失（$\beta$ 是平衡系数）

3. **反向传播**：
   - 通过重参数化的 $z$ 计算梯度
   - 更新编码器和解码器的参数

## VAE的直观理解

从信息论的角度，VAE可以理解为一个信息瓶颈（Information Bottleneck）：

- **编码器**：压缩输入信息到隐变量，但要保留足够的信息用于重构
- **KL正则化**：限制隐变量携带的信息量，防止过拟合
- **解码器**：从压缩的表示中恢复原始信息

从几何角度，VAE学习了一个从数据空间到隐空间的光滑映射：

- **连续性**：相似的输入被映射到隐空间中相近的位置
- **可插值性**：隐空间中两点之间的插值对应有意义的生成结果
- **解耦性**：理想情况下，隐变量的不同维度对应数据的不同属性

## VAE的优势与局限

**优势：**

1. **理论基础扎实**：基于变分推断的严格数学框架
2. **稳定训练**：相比GAN，训练过程更稳定，不存在模式坍塌问题
3. **可解释的隐空间**：隐变量有明确的概率解释
4. **支持推断**：可以将新数据编码到隐空间
5. **易于扩展**：框架灵活，易于加入各种先验知识

**局限：**

1. **生成质量**：由于使用像素级重构损失，生成的图像往往比较模糊
2. **后验坍塌**：KL项可能导致编码器忽略输入，退化为先验分布
3. **表达能力受限**：简单的高斯假设可能无法捕捉复杂的数据分布
4. **重构与生成的权衡**：重构质量和生成多样性之间存在固有矛盾

## VAE的重要变体

为了克服原始VAE的局限性，研究者们提出了许多改进版本：

### 1. β-VAE
通过调整KL项的权重 $\beta$，在重构质量和隐变量解耦之间进行权衡：
$$\mathcal{L}_{\beta\text{-VAE}} = -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \beta \cdot \text{KL}(q_\phi(z|x) \| p(z))$$

- 当 $\beta > 1$ 时，鼓励更强的解耦
- 当 $\beta < 1$ 时，提高重构质量

### 2. Conditional VAE (CVAE)
引入条件信息 $c$（如类别标签），实现可控生成：
$$q_\phi(z|x,c), \quad p_\theta(x|z,c)$$

应用场景：
- 条件图像生成
- 半监督学习
- 多模态学习

### 3. VQ-VAE (Vector Quantized VAE)
使用离散的隐变量和向量量化技术：
- 解决后验坍塌问题
- 提高生成质量
- 支持自回归生成

### 4. Hierarchical VAE
使用多层隐变量，捕捉数据的层次结构：
$$z = \{z_1, z_2, ..., z_L\}$$
其中高层变量捕捉全局信息，低层变量捕捉局部细节。

### 5. Adversarial Variational Bayes (AVB)
结合GAN的思想，使用判别器来隐式定义变分分布：
- 避免了高斯假设的限制
- 提高了后验分布的表达能力

## VAE在实际应用中的技巧

### 1. 防止后验坍塌
- **KL退火（KL Annealing）**：训练初期减小KL项权重，逐渐增加
- **自由比特（Free Bits）**：为每个隐变量维度设置最小信息量
- **跳跃连接**：在解码器中加入跳跃连接，减少对隐变量的依赖

### 2. 提高生成质量
- **感知损失**：使用预训练网络的特征匹配损失代替像素损失
- **对抗训练**：加入判别器，提高生成样本的真实感
- **自回归解码器**：使用PixelCNN等自回归模型作为解码器

### 3. 改进隐空间
- **归一化流（Normalizing Flows）**：增强后验分布的表达能力
- **混合高斯先验**：使用更灵活的先验分布
- **信息最大化**：鼓励隐变量之间的独立性

## VAE与其他生成模型的比较

| 特性 | VAE | GAN | 扩散模型 |
|------|-----|-----|----------|
| 理论基础 | 变分推断 | 博弈论 | 随机过程 |
| 训练稳定性 | 高 | 低 | 高 |
| 生成质量 | 中等 | 高 | 最高 |
| 推断能力 | 支持 | 不支持 | 部分支持 |
| 训练速度 | 快 | 中等 | 慢 |
| 可解释性 | 高 | 低 | 中等 |

## 总结与展望

VAE作为一种基于变分推断的生成模型，提供了一个优雅的理论框架来学习数据的潜在表示。通过最大化证据下界，VAE同时实现了数据的压缩和生成，在许多应用场景中展现出独特的优势。

尽管VAE在生成质量上可能不如GAN或扩散模型，但其稳定的训练过程、可解释的隐空间和灵活的框架使其在实际应用中仍然占有重要地位。特别是在需要推断能力、可控生成或结构化隐空间的场景中，VAE及其变体仍然是首选方案。

随着深度学习技术的发展，VAE也在不断进化。从简单的高斯VAE到复杂的层次化、离散化变体，从纯生成模型到与其他方法的混合模型，VAE的研究仍在继续。未来，我们期待看到更多创新的VAE变体，在保持其理论优势的同时，进一步提升生成质量和应用范围。

正如我们在本系列文章中看到的，每种生成模型都有其独特的优势和适用场景。理解这些模型背后的数学原理和设计思想，将帮助我们在实际问题中做出更好的选择。


